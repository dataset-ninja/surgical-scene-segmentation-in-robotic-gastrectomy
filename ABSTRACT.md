Authors introduce the Surgical Scene Segmentation in **Robotic Gastrectomy with Real and Synthetic Data** dataset, addressing the shortage of public data in surgical vision research. It combines real and synthetic data for instance and semantic segmentation model training, with synthetic data improving model performance for low-performance classes. Authors also implement class-balanced frame sampling to handle class imbalance and reduce redundant frames and data labeling costs, making this dataset valuable not only for gastrectomy but also for other minimally invasive surgeries.

Recent advances in deep neural network architectures and learning large-scale datasets have signiﬁcantly improved model performance. Because of that, many large-scale datasets have been published for computer vision applications such as [natural scene recognition](https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48) or [autonomous driving](https://arxiv.org/abs/1604.01685), [remote sensing](https://openaccess.thecvf.com/content_WACV_2020/papers/Bondi_BIRDSAI_A_Dataset_for_Detection_and_Tracking_in_Aerial_Thermal_WACV_2020_paper.pdf), [computer-assisted design](https://arxiv.org/abs/1812.06216), [medical imaging](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_IntrA_3D_Intracranial_Aneurysm_Dataset_for_Deep_Learning_CVPR_2020_paper.pdf), etc. Unlike other computer vision applications having abundant datasets, surgical vision research always has had a problem of public data shortages. Fortunately, several surgical vision datasets were published and encouraged the research. However, building those large-scale surgical vision datasets cost more than common datasets due to the rareness of surgery videos and the diﬃculties of annotations. Because of that, several researchers conducted image synthesis research for surgical vision to decrease data generation costs. However, the previous works had limited results for real-world applications with simple simulators, including only a few organs and surgical tools and outdated segmentation models to evaluate the quality of the image. In addition, none of the research provided real image data, including annotations, which is necessary for the open research. Therefore, authors release a new Surgical Scene Segmentation in Robotic Gastrectomy with Real and Synthetic Data dataset to encourage further study and provide novel methods with extensive experiments for surgical scene segmentation using semantic image synthesis with a more complex virtual surgery environment.

Authors compare state-of-the-art instance and semantic segmentation models trained with real data and synthetic data. First, authors created three cross-validation datasets considering demographic and clinical information from 40 cases of real surgical videos of distal gastrectomy for gastric cancer with the da Vinci Surgical System (dVSS). Authors annotated six organs and 14 surgical instruments divided into 24 instrument parts according to head, wrist, and body structures commonly appearing during the surgery. Authors also introduced a class-balanced frame sampling to suppress many redundant classes and to reduce unnecessary data labeling costs. Second, authors created a virtual surgery environment in the Unity engine with 3D models of ﬁve organs from real patient CT data and 11 surgical instruments from actual measurements of the da Vinci surgical instruments. The environment provides cost-free pixel-level annotations enabling segmentation. Third, authors used state-of-the-art semantic image synthesis models to convert authors' 3D simulator photo-realistically. Lastly, authors created ten combinations of real and synthetic data for each cross-validation set to train/evaluate segmentation models. The analysis showed interesting results: synthetic data improved low-performance classes and was very eﬀective for Mask AP improvement while improving the segmentation models overall. Finally, authors release the ﬁrst large-scale semantic/instance segmentation dataset, including both real and synthetic data that can be used for visual object recognition and image-to-image translation research for gastrectomy with the dVSS. However, authors' method is not only limited to gastrectomy but can be generalized into all minimally invasive surgeries such as laparoscopic and robotic surgery.

<img src="https://i.ibb.co/k6WRTn0/Screenshot-2023-10-06-160547.png" alt="image" width="800">

<span style="font-size: smaller; font-style: italic;"> The schematic diagram of surgical scene segmentation using semantic image synthesis with a virtual surgery environment. SPADE [19] and SEAN [20] translate synthetic data generated from a virtual surgery environment photorealistically and are used for training surgical scene segmentation models. </span>

The contribution of authors' work is summarized as follows: 
- Authors release the ﬁrst large-scale instance and semantic segmentation dataset, including both real and synthetic data that can be used for visual object recognition and image-to-image translation research for gastrectomy with the dVSS. 
- Authors systematically analyzed surgical scene segmentation using semantic image synthesis with state-of-the-art models with ten combinations of real and synthetic data. 
- Authors found interesting results that synthetic data improved low-performance classes and was very eﬀective for Mask AP improvement while improving the segmentation models overall.

Authors collected 40 cases of real surgical videos of distal gastrectomy for gastric cancer with the da Vinci Surgical System (dVSS), approved by an institutional review board at the medical institution. In order to evaluate generalization performance, authors created three cross-validation datasets considering demographic and clinical variations such as gender, age, BMI, operation time, and patient bleeding. Each cross-validation set consists of 30 cases for train/validation and 10 cases for test data.

Authors list ﬁve organs (Gallbladder, Liver, Pancreas, Spleen, and Stomach) and 13 surgical instruments that commonly appear from surgeries (Hamonic Ace; HA, Stapler, Cadiere Forceps; CF, Maryland Bipolar Forceps; MBF, Medium-large Clip Applier; MCA, Small Sclip Applier; SCA, Curved Atraumatic Graspers; CAG, Suction, Drain Tube; DT, Endotip, Needle, Specimenbag, Gauze). They classify some rare organs and instruments as “other tissues” and “other instruments” classes. The surgical instruments consist of robotic and laparoscopic instruments and auxiliary tools mainly used for robotic subtotal gastrectomy. In addition, authors divide some surgical instruments according to their head, H, wrist; W, and body; B structures, which leads to 24 classes for instruments in total. 

It is widely known that neural networks struggle to learn from a class-imbalanced dataset, which also happens in surgical vision. The problem can be alleviated by loss functions or data augmentation techniques; however, a data sampling method in the data generation step can also easily release it. Authors introduce a class balanced frame sampling, considering a class distribution while creating the dataset. They select major scenes(frames) to learn while keeping a class distribution. Authors select the same number of frames for each instrument and organ in each surgery video. This approach also reduces unnecessary data labeling costs by suppressing many redundant frames, which originate from the temporal redundant characteristics of the video.
